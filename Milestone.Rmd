---
title: "Capstone Milestone report"
author: "Michal Dzwolak"
date: "10 January 2018"
output: html_document
---

The goal of this project is to develop next word predictive shiny application. Following document will present analysis of given data based on which mentioned application will be developed. In this capstone i will be applying data science in the area of natural language processing.

Datasets:

- USBlogs

- USNews

- USTwitter

Summary of three datasets:

###Blogs dataset
```{r blog, warning=FALSE, message=FALSE, echo=FALSE}
library(ggplot2)
library(tm)
library(textreg)
library(tidytext)
library(dplyr)
library(stringi)
library(knitr)

#Blogs dataset
#read blogs
blogs <- "C:/Users/m.dzwolak/Documents/Capstone/final/en_US/en_US.blogs.txt"
con1 <- file(blogs,open="r")
lineblogs <- readLines(con1)
close(con1)

#read blogs
news <- "C:/Users/m.dzwolak/Documents/Capstone/final/en_US/en_US.news.txt"
con2 <- file(news,open="r")
linenews <- readLines(con2)
close(con2)

#read twitter
twitter <- "C:/Users/m.dzwolak/Documents/Capstone/final/en_US/en_US.twitter.txt"
con3 <- file(twitter,open="r")
linetwitt <- readLines(con3)
close(con3)

kable(data.frame(File = "Blog", Numbers = stri_stats_general(lineblogs)))
summary(lineblogs)

#xlim changed to 500 to be able to see clear words count distribution 
qplot(stri_count_words(lineblogs), xlim = c(0, 500))
```

###News dataset
```{r news, warning=FALSE, message=FALSE, echo=FALSE}
#News dataset
kable(data.frame(File = "News", Numbers = stri_stats_general(linenews)))
summary(linenews)

qplot(stri_count_words(linenews), xlim = c(0, 500))
```

###Twitter dataset
```{r twit, warning=FALSE, message=FALSE, echo=FALSE}
#Twitter dataset
kable(data.frame(File = "Twitter", Numbers = stri_stats_general(linetwitt)))
summary(linetwitt)

qplot(stri_count_words(linetwitt), xlim = c(0, 500))
```

###Sampling

To decrease amount of data i applied sampling based on individual length of dataset. 
```{r sampl, warning=FALSE, message=FALSE, echo=TRUE}
#sampling
set.seed(1234)
lineblogss = sample(lineblogs, length(lineblogs)*0.05)
linenewss = sample(linenews, length(linenews)*0.05)
linetwitts = sample(linetwitt, length(linetwitt)*0.05)

#combine data in to one dataset
data = c(lineblogss, linenewss, linetwitts)
```

###Summary of final dataset
```{r summ, warning=FALSE, message=FALSE, echo=FALSE}
#dataset before cleaning
kable(stri_stats_general(data))
summary(data)
```

###Cleaning dataset
```{r clean, warning=FALSE, message=FALSE, echo=TRUE}
corpus <- VCorpus(VectorSource(data))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus <- tm_map(corpus, toSpace, "(f|ht)tp(s?)://(.*)[.][a-z]+")
corpus <- tm_map(corpus, toSpace, "@[^\\s]+")
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, PlainTextDocument)
```

###Unigrams analysis
```{r uni, warning=FALSE, message=FALSE, echo=FALSE}
#conversion in to character vector
mytext = convert.tm.to.character(corpus)

patterns = c('Á', 'ê','ã','ç', 'à', 'ú', 'ü', 'â', 'ã', '¢')

unigram = data_frame(text = mytext) %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>%
  group_by(word) %>% 
  count(word, sort = TRUE) %>%
  mutate(badword = ifelse(grepl(paste(patterns, collapse="|"), word)==TRUE | grepl("fuck", word)==TRUE,1,0)) %>% 
  filter(badword==0) %>%
  select(word, n)

#delete unigrams with a frequency below mean
unigram = unigram %>% filter(n>mean(unigram$n))

#new summary
summary(unigram)

#most frequent unigrams graph
plot1 = ggplot(head(unigram,15), aes(x=reorder(word,-n), y=n)) + 
  geom_col() + 
  theme_light() +
  ylab("Count") + 
  xlab("Word") + 
  ggtitle("Most frequent unigrams")
  
print(plot1)
```

###Bigrams analysis
```{r big, warning=FALSE, message=FALSE, echo=FALSE}
#most frequent twograms graph
bigram = data_frame(text = mytext) %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  group_by(bigram) %>% 
  count(bigram, sort = TRUE) %>%
  mutate(badword = ifelse(grepl(paste(patterns, collapse="|"), bigram)==TRUE | grepl("fuck", bigram)==TRUE,1,0)) %>%
  filter(badword==0) %>%
  select(bigram, n)

plot2 = ggplot(head(bigram,15), aes(x=reorder(bigram,-n), y=n)) + 
  geom_col() + 
  theme_light() +
  ylab("Count") + 
  xlab("Bigram") + 
  ggtitle("Most frequent bigrams")

print(plot2)
```

###Trigrams analysis
```{r tri, warning=FALSE, message=FALSE, echo=FALSE}
#most frequent trigrams graph
trigram = data_frame(text = mytext) %>% 
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  group_by(trigram) %>% 
  count(trigram, sort = TRUE) %>%
  mutate(badword = ifelse(grepl(paste(patterns, collapse="|"), trigram)==TRUE | grepl("fuck", trigram)==TRUE,1,0)) %>%
  filter(badword==0) %>%
  select(trigram, n)

plot3 = ggplot(head(trigram,15), aes(x=reorder(trigram,-n), y=n)) + 
  geom_col() + 
  theme_light() +
  ylab("Count") + 
  xlab("Trigram") + 
  ggtitle("Most frequent trigrams") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

print(plot3)
```

###Creating new datasets

write.table(unigram, file = "unigram.txt", row.names = FALSE)

write.table(bigram, file = "bigram.txt", row.names = FALSE)

write.table(trigram, file = "trigram.txt", row.names = FALSE)


###Further steps

- create predictive alhorithm where i will apply backoff model to estimate the probability of unobserved n-grams.
- apply autocorrections for words inputed with a misstake (optional).
- create shiny graphic interface.
- upload final shiny predictive application.